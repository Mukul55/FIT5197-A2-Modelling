---
title: "FIT5197 Assessment 2"
author: "Mukul Gupta, 29873150"
date: "11 October 2018"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Task A: Modelling - Classification


```{r}
# loading the  dataset LoanData.csv
getwd()
setwd("G:/My Drive/Monash/Sem 1/FIT5197/Assignment 2/")
loan_data <- read.csv("LoanData.csv",header=TRUE)
str(loan_data)
summary(loan_data)
```

### A.1

```{r}
# viewing the structure of loan-status
str(loan_data$loan_status)
# summarising the loan-status
summary(loan_data$loan_status)
```

##### After summarising, we see that loan_status is a categorical variable with 2 values "Charged Off" and "Fully Paid". While dealing with binary categorical variables, we model using logistic regression instead of linear regression. In logistic regression, we find the probability of the outcome based on the value of explanatory variables. Linear regression is used to predict numerical and continuous variable and can produce  values less than 0 and more than 1. Thus, it cannot be used with binary categorical variable.


### A.2

```{r}
# viewing the structure of grade
str(loan_data$grade)
# plotting the loan_status vs grade
plot(loan_data$loan_status ~ loan_data$grade)

# converting grade to numeric
temp_grade<-as.numeric(loan_data$grade)
# viewing the structure of grade and plotting it with loan_status
str(temp_grade)
plot(loan_data$loan_status ~ temp_grade)
```

##### When we convert categorical variable to numeric  variable, we decrease the number of parameters in the logistic model. In case of categorical variable, each level will be treated as separate predictors while in case of numeric variable, there will be only one predictor. "grade" variable has 7 levels. After converting "grade" variable to numeric, it contains  integer values between 1 to 7. We see that "grade" variable doesn't follow a pattern after converting to numeric. There is no significant change and hence it doesn't affect the model whether "grade" is numeric or "categorical"


### A3

```{r}
# creating logistic regression model using glm function
log_model <- glm(loan_status ~ ., family = binomial, data = loan_data)

# summary of the model
summary(log_model)
```
#####Estimate: It is the maximum likelihhod estimate that maximises the likelihood function over the parameter values. 

#####Standard error: It is the estimate of standard deviation of each of the coefficient. It is said as the measure of the precision of the coefficient.

##### (Ref: http://documentation.statsoft.com/portals/0/formula%20guide/Logistic%20Regression%20Formula%20Guide.pdf)

###There more rows in the coefficients table than there are variables in the data. This is because for every categorical variable if there are n levels, n-1 coefficients are taken because of the concept of dummy variable. Hence, there are more rows.



### A4

```{r}
# summary of the model
summary(log_model)
```

#####We see that int_rate (interest rate) is inversely proportional to the value predicted. By intuition, we say that higher interest rate should corespond to defaulters i.e.  status as "Charged Off". Therefore, lower rate would correspond to "Fuly Paid". Also, annual_inc (annual income) should be directly proportional to the chances of "Fully Paid". Hence we say that in the model we trained, the probabilities represent whether the loan is paid i.e. loan status= "Fully Paid" In R, the logistic function takes the first argument as 0 and second argument as 1. In the factor loan status, ="Charged Off" is 0 and "Fully Paid" is 1. Hence, it is confirmedthat w e predict the probabilites of loan status= "Fully Paid".

```{r}
# reading the test data
loan_test_data <- read.csv("LoanData_test.csv",header=TRUE)
# predicting the loan_status on the test_data
prob = predict(log_model, loan_test_data, type = 'response')
```


### A5

```{r}
# if value from logistic model is greater than 0.5 it is taken as "Fully Paid" and else 
# is taken as "Charged Off" for values less than equal to 0.5
loan_test_data$predict_loan_status <- as.factor(ifelse(prob > 0.5, "Fully Paid", "Charged Off"))

# creating the confusion matrix
conf.matrix <- as.matrix(table('Actual'=loan_test_data$loan_status, 'Prediction'=loan_test_data$predict_loan_status))
conf.matrix
# calculating the accuracy
diag <- diag(conf.matrix)
accuracy <- sum(diag)/nrow(loan_test_data)
cat("Accuracy: ", accuracy*100, "%")
```

$$
\begin{aligned}
Accuracy\ &=\ \frac{True\ positives(TP)\ +\ True\ negatives(TN)}{Total(TP+TN+FP+FN)} \\
&= \frac{85\ +\ 0}{100} \\
&= 85\%
\end{aligned}
$$
#### We correctly predict 85% of the test data


### A6

```{r}
# reading the test data
loan_test_data <- read.csv("LoanData_test.csv",header=TRUE)

loan_test_data$predict_loan_status2 <- rep(1,100)
loan_test_data$predict_loan_status2 <- as.factor("Fully Paid")
str(loan_test_data$predict_loan_status2)

# creating the confusion matrix
conf.matrix2 <- as.matrix(table('Actual'=loan_test_data$loan_status,'Prediction'=loan_test_data$predict_loan_status2))
conf.matrix2
# calculating the accuracy
accuracy <- sum(conf.matrix2[2])/nrow(loan_test_data)
cat("Accuracy: ", accuracy*100, "%")
```
#### This model accurately predicts 86% of the test data

#### Though accuracy of this model is greater and the model is relatively simple, we shouldn't use this model. This model doesn't consider the cases when loan is defaulted i.e. loan status="Charged Off". This means, the model always predicts that loan is paid i.e. loan status= "Fully Paid". This will cause problems as it doesn't take into account the major problem i.e. it doesn't predict when the loan is defaulted.

## TASK B: Modelling - Regression

### B.1 

```{r}
# loading the auto_mpg_clean_train.csv dataset
auto_mpg_data <- read.csv("auto_mpg_train.csv",header=TRUE)
# checking the structure of dataset
str(auto_mpg_data)
# replacing "?" with NA value
auto_mpg_data[auto_mpg_data == "?"]<- NA
# converting "horsepower" value to numeric
auto_mpg_data$horsepower <- as.numeric(as.character(auto_mpg_data$horsepower))
# replacing NA value with mean of those "horsepower" value where "cylinders"=4
auto_mpg_data$horsepower <- ifelse(is.na(auto_mpg_data$horsepower) & (auto_mpg_data$cylinders==4), mean(auto_mpg_data$horsepower[auto_mpg_data$cylinders==4], na.rm=TRUE), auto_mpg_data$horsepower)
# replacing NA value with mean of those "horsepower" value where "cylinders"=6
auto_mpg_data$horsepower <- ifelse(is.na(auto_mpg_data$horsepower) & (auto_mpg_data$cylinders==6), mean(auto_mpg_data$horsepower[auto_mpg_data$cylinders==6], na.rm=TRUE), auto_mpg_data$horsepower)

```

#####Imputation of missing horsepower in the data above is done using the mean of the other "horsepower" values for a given "cylinder" value.

##### Clean dataset given "auto_mpg_clean_train.csv" is used for the rest questions

### B.2

```{r}
# loading the auto_mpg_clean_train.csv dataset
auto_mpg_data <- read.csv("auto_mpg_clean_train.csv",header=TRUE)
# for finding relationships among different variables of the data
pairs(mpg ~ cylinders + displacement + horsepower + weight + acceleration + model.year + origin + car.name, data = auto_mpg_data)
```

##### Based on the plot, I would choose  "cylinders", "displacement", "horsepower", "weight", "acceleration", "model.year", "origin" to predict "mpg"."car.name" doesn't show any relationship with "mpg" through the graph.

### B.3

```{r}
# creating a linear model using all predictors except "car.name"
linear_model <- lm(mpg ~ cylinders + displacement + horsepower + weight + acceleration + model.year + origin, data = auto_mpg_data)
# summarizing the model
summary(linear_model)

```

##### $R^2$ value: It measures the goodness of the fit (the higher the better with maximum value 1). It measures the amount of variability our model can explain. Our model can explain 82.29% of the mpg. In multiple linear regression, $R^2$ is adjusted with the number of variables modelled. This is because as number of predictors increase, $R^2$ also increases. Therefore, adjusted $R^2$ provides a better metric than $R^2$

##### p-value: p-value is calculaed for each parameter and tests the null hypothesis that the value of that parameter is 0 (it has no effect on prediction). Low p-value suggests that we can reject the null hypothesis, which means that the parameter is actually important in predicting the response variable and should be added in our model. In our model "horsepower", "cylinders", "model.year", "origin", "weight" have low p-value and hence are good variables to add in our model.


### B4

```{r}
# loading the test file
auto_mpg_test <- read.csv("auto_mpg_test.csv",header=TRUE)
# predicting values using the linear model and storing them in a new column predicted_mpg in the test data
auto_mpg_test$predicted_mpg <- predict(linear_model, auto_mpg_test)

# finding and printing the mean squared error
mse <- mean(auto_mpg_test$mpg - auto_mpg_test$predicted_mpg)^2
cat("MSE:",mse)
```

### B5

```{r}
# First used "horsepower", "weight", "model.year", "origin" as the variables for the model.
# This didn't improve the R-square value
# Therefore, used ("horsepower^2") as an extra parameter to improve the model

# creating a new model using variables "horsepower", squaring the horsepower("horsepower^2"), "weight", "model.year", "origin"
linear_model_new <- lm(mpg ~ horsepower + I(horsepower^2) + weight + model.year + origin, data = auto_mpg_data)
# summarising the new model
summary(linear_model_new)
# predicting using the new model
auto_mpg_test <- read.csv("auto_mpg_test.csv",header=TRUE)
auto_mpg_test$predicted_mpg <- predict(linear_model_new, auto_mpg_test)

# finding and printing the mean squared error
mse <- mean(auto_mpg_test$mpg - auto_mpg_test$predicted_mpg)^2
cat("MSE:",mse)

summary(linear_model_new)
```
#### MSE decreased to 0.6575391
#### Adjusted R-square is 0.8477 
#### As MSE is decreased and adjusted R-square has increased giving us an improved model




## Task C: Sampling



## C1

$$ p(x) = \frac{1}{1 − e^{-4}}2e^{−2x} ,\:for \:\:x\:\epsilon\:[0,2] $$
#### Inverse sampling
$$ 
\begin{aligned}
CDF = F(x) &= \int_{0}^x \frac{1}{1 − e^{-4}}2e^{−2x}dx \\
&= \frac{1}{1 − e^{-4}} (1-e^{−2x})
\end{aligned}
$$
By taking the inverse of cdf we find the quantile function q(p) which is given by
$$ 
q(p) = - \frac{1}{2}log(p(e^{-4}-1)+1),\;for\:p\:\epsilon\:[0,1]
$$

#### We find samples using this inverse cdf or quantile function 

```{r}
# creating inverse_cdf function
inverse_cdf <- function(p) {
    return (-0.5 * (log(p * (exp(-4) - 1) + 1)))
}

# set seed to some value to reproduce the output
set.seed(111)

# samples are found by applying inverse_cdf function to 200 samples ranging from 0 to 1
samples <- sapply(runif(200),inverse_cdf)

# histogram is plotted of the samples
hist(samples,
      main = "200 Sample Values using Inverse Sampling",
      xlab = "X",
      col = "blue", breaks = 10)

```

## C2

#### 1. Joint probability distribution
$$ 
\begin{aligned}
p(cloudy, sprinkler, rain, wetgrass) &=  p(C = cloudy, S = sprinkler, R = rain, W = wetgrass) \\
&= p(C)\ p(R|C)\ p(S|C)\ p(W|S,R) \\
\end{aligned}
$$

#### 2.
#### No variable is independent of Rain(R). Cloudy(C) is ancestor of R, so C and R are conditionally dependent. Sprinkler(S) and Rain(R) have common ancestor C, therefore S and R are onditionally dependent. Wetgrass(W) is successor of Rain(R), therefore W and R are related. 
#### Cloudy(C) is independent of Rain.



## C3
$$ 
\begin{aligned}
p(S|C,R,W) &\propto\ p(S|C)\ p(W|S,R) \\
&=  \frac{p(S|C)\ p(W|S,R)}{p(S = F|C)\ p(W|S=F,R) + p(S=T|C)\ p(W|S = T,R)} \\
\end{aligned}
$$

```{r}
# initialising the prob values
cpt_c = c(0.5, 0.5)
cpt_s_given_c <- matrix(c(0.5, 0.5, 0.9, 0.1), 2, 2, byrow = F)
cpt_r_given_c <- matrix(c(0.8, 0.2, 0.2, 0.8), 2, 2, byrow = F)
cpt_w_given_sr <- matrix(c(1, 0.1, 0.1, 0.01, 0, 0.9, 0.9, 0.99), 2, 4, byrow = T)

# creating p_s_given_crw function that returns p(S|C,R,W)
p_s_given_crw <- function(S,C,R,W){
    if (R == T){
        num_index <- ifelse(S == 0, 3, 4)
        den_index <- c(3, 4)
    }
    else if (R == F){
        num_index <- ifelse(S == 0, 1, 2)
        den_index <- c(1, 2)
    }
    
    num <- cpt_s_given_c[S+1, C+1] * cpt_w_given_sr[W+1, num_index]
    den <- sum(cpt_s_given_c[, C+1] * cpt_w_given_sr[W+1, den_index])
    
    return(num/den)
}

cat("p(S = TRUE| C = FALSE, R = TRUE, W = FALSE): ", p_s_given_crw(S = TRUE, C = FALSE, R = TRUE, W = FALSE),"\n")
cat("p(S = TRUE| C = TRUE, R = TRUE, W = TRUE): ", p_s_given_crw(S = TRUE, C = TRUE, R = TRUE, W = TRUE))
```

## C4

####Using Gibbs sampling to estimate p(C = T|W =T)


##### We need to calculate conditional probability of C given W. To find these probabilities, we find the conditional probabilities of each variable given the rest variables on which it is dependent  .
##### We need probability of 
##### C given S,R
##### S given C,R,W
##### W given S,R
##### S given C,W

##### We assume the initial probability value for first iteration. We then start finding the next values by comparing the previous values with a uniform random number generated from 0 to 1. If the randomly generated number is less the probbility, we put the value as 0 else 1. After running the same procedure 1000 times, we genrate an estimate. We remove few samples (100 in question c5) to get a decent estimate. 

## C5

#### Implementation of Gibbs sampling to estimate p(C = T|W =T)


```{r}

# finding all the conditional probabilities
# calculating p_r_given_csw
p_r_given_csw <- function(c, s, w) {
  if (s == 1) {
    index <- c(2,4)
  }
  else if (s==0) {
    index <- c(1,3)
  } 
  return (cpt_r_given_c[,c+1]*cpt_w_given_sr[w+1,index]/sum(cpt_r_given_c[,c+1]*cpt_w_given_sr[w+1,index]))
}

# calculating p_s_given_crw
p_s_given_crw <- function(c, r, w) {
  if (r == 1) {
    index = c(3, 4)
  }
    else if (r == 0) {
    index = c(1, 2)
  }
  return(cpt_s_given_c[, c + 1] * cpt_w_given_sr[w + 1, index]/sum(cpt_s_given_c[, c + 1] * cpt_w_given_sr[w + 1, index]))
}

# calculating p_c_given_sr
p_c_given_sr <- function(s, r) {
  return(cpt_c * cpt_s_given_c[s + 1, ] * cpt_r_given_c[r + 1, ]/sum(cpt_c * cpt_s_given_c[s + 1, ] * cpt_r_given_c[r + 1, ]))
}

# calculating p_w_given_sr
p_w_given_sr <- function(s, r) {
  if ((s == 0) && (r == 0)) {
    index <- 1
  } else if ((s == 1) && (r == 0)) {
    index <- 2
  } else if ((s == 0) && (r == 1)) {
    index <- 3
  } else if ((s == 1) && (r == 1)){
    index <- 4
  }
  return(cpt_w_given_sr[, index])
}


```

```{r}
# Sampling
# Initialise the matrix containing 
sample_matrix <- matrix(0, 1000, 4)
colnames(sample_matrix) <- c('C','R','S','W')

# Initialize first row to 0
sample_matrix[1,] <- 0

# setting the seed
set.seed(111)

# as we have first sample as 0,0,0,0
# we find the next samples
for (ind in 2:1000){
  # get a uniform random number
  c_num <- runif(1,0,1)
  # calculating probability of C given S,R
  prob_1 <- p_c_given_sr(sample_matrix[ind-1,"S"],sample_matrix[ind-1,"R"])
  # if the random number is less than prob of C=False then next value is 0 else it is 1
  sample_matrix[ind,"C"] <- ifelse(c_num < prob_1[1],0,1) # next sample is false, if random value is less than probability of false for event C; true otherwise
  
  # get a uniform random number
  r_num <- runif(1,0,1)
  # calculating probability of R given C,S,R
  prob_2 <- p_r_given_csw(sample_matrix[ind-1,"C"],sample_matrix[ind-1,"S"],sample_matrix[ind-1,"W"])
  # if the random number is less than prob of r=False then next value is 0 else it is 1
  sample_matrix[ind,"R"] <- ifelse(r_num < prob_2[1],0,1)
  
  # get a uniform random number
  s_num <- runif(1,0,1)
  # calculating probability of S given C,R,W
  prob_3 <- p_s_given_crw(sample_matrix[ind-1,"C"],sample_matrix[ind-1,"R"],sample_matrix[ind-1,"W"])
  sample_matrix[ind,"S"] <- ifelse(s_num < prob_3[1],0,1)
 
  # get a uniform random number
  w_num <- runif(1,0,1)
  # calculating probability of W given S,R
  prob_4 <- p_w_given_sr(sample_matrix[ind-1,"S"],sample_matrix[ind-1,"R"])
  sample_matrix[ind,"W"] <- ifelse(w_num < prob_4[1],0,1)
}

# dircard first 100 samples
# we take remaining 900 values
sample_matrix_temp <- sample_matrix[101:1000,]
# initialising a matrix to store every 10th sample
# storing every 10th sample 
final_matrix <- matrix(-1,nrow = 90,ncol = 4)
colnames(final_matrix) <- c("C","R","S","W")

# storing every tenth element in the sample_matrix
row_count <-1
ind=1
for (ind in 1:nrow(sample_matrix_temp)){
  # if the remainder on dividing with 10 is 0 then we add that element    
  if (ind%%10 == 0) {
    final_matrix[row_count,] <- sample_matrix_temp[ind,]
    row_count <- row_count+1
}

}
# finding the conditional probability P(C = T|W = T)
final_matrix  <- as.data.frame(final_matrix)
cpt_c_given_w <- table(final_matrix[,c("W", "C")])/nrow(final_matrix)
# printing the conditional probability P(C = T|W = T)
cat("P( C = TRUE | W = TRUE ) is:", cpt_c_given_w[2,2],".\n")

```
